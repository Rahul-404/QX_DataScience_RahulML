{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§πüèª‚Äç‚ôÄÔ∏è Regularization in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major aspects of training your machine learning model is avoiding overfitting. The model will have a low accuracy if it is overfitting. This happens because your model is trying too hard to capture the noise in your training dataset. <b>By noise we mean the data points that don‚Äôt really represent the true properties of your data, but random chance.</b> Learning such data points, makes your model more flexible, at the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The concept of balancing bias and variance, is helpful in understanding the phenomenon of overfitting.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://github.com/Rahul-404/QX_DataScience_RahulML/blob/master/Unit8_Model_Improvement_%26_Validation/1_Balancing_Bias_and_Variance_to_Control_Errors.ipynb'>Balancing Bias and Variance to Control Errors in Machine Learning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>One of the ways of avoiding overfitting is using cross validation, that helps in estimating the error over test set, and in deciding what parameters work best for your model.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://github.com/Rahul-404/QX_DataScience_RahulML/blob/master/Unit8_Model_Improvement_%26_Validation/2_Cross-Validation.ipynb'>Cross-Validation in Machine Learning</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This article will focus on a technique that helps in avoiding overfitting and also increasing model interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a form of regression, that constrains/ regularizes or shrinks the coefficient estimates towards zero. In other words, <b>this technique discourages learning a more complex or flexible model, so as to avoid the risk of overfitting.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple relation for linear regression looks like this. Here Y represents the learned relation and Œ≤ represents the coefficient estimates for different variables or predictors(X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y ‚âà Œ≤0 + Œ≤1X1 + Œ≤2X2 + ‚Ä¶+ Œ≤pXp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting procedure involves a loss function, known as residual sum of squares or RSS. The coefficients are chosen, such that they minimize this loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/reg_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this will adjust the coefficients based on your training data. If there is noise in the training data, then the estimated coefficients won‚Äôt generalize well to the future data. This is where regularization comes in and shrinks or regularizes these learned estimates towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/reg_2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above image shows ridge regression, where the <b>RSS is modified by adding the shrinkage quantity.</b> Now, the coefficients are estimated by minimizing this function. Here, <b>Œª is the tuning parameter that decides how much we want to penalize the flexibility of our model.</b> The increase in flexibility of a model is represented by increase in its coefficients, and if we want to minimize the above function, then these coefficients need to be small. This is how the Ridge regression technique prevents coefficients from rising too high. Also, notice that we shrink the estimated association of each variable with the response, except the intercept Œ≤0, This intercept is a measure of the mean value of the response when xi1 = xi2 = ‚Ä¶= xip = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Œª = 0, the penalty term has no eÔ¨Äect, and the estimates produced by ridge regression will be equal to least squares. However, as <b>Œª‚Üí‚àû, the impact of the shrinkage penalty grows, and the ridge regression coeÔ¨Écient estimates will approach zero.</b> As can be seen, selecting a good value of Œª is critical. Cross validation comes in handy for this purpose. The coefficient estimates produced by this method are <b>also known as the L2 norm.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The coefficients that are produced by the standard least squares method are scale equivariant,</b> i.e. if we multiply each input by c then the corresponding coefficients are scaled by a factor of 1/c. Therefore, regardless of how the predictor is scaled, the multiplication of predictor and coefficient(XjŒ≤j) remains the same. <b>However, this is not the case with ridge regression, and therefore, we need to standardize the predictors or bring the predictors to the same scale before performing ridge regression.</b> The formula used to do this is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/reg_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/reg_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso is another variation, in which the above function is minimized. Its clear that <b>this variation differs from ridge regression only in penalizing the high coefficients.</b> It uses |Œ≤j|(modulus)instead of squares of Œ≤, as its penalty. In statistics, this is <b>known as the L1 norm.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at above methods with a different perspective. The ridge regression can be thought of as solving an equation, where summation of squares of coefficients is less than or equal to s. And the Lasso can be thought of as an equation where summation of modulus of coefficients is less than or equal to s. Here, s is a constant that exists for each value of shrinkage factor Œª. <b>These equations are also referred to as constraint functions.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Consider their are 2 parameters in a given problem.</b> Then according to above formulation, the <b>ridge regression is expressed by Œ≤1¬≤ + Œ≤2¬≤ ‚â§ s.</b> This implies that ridge regression coefficients have the smallest RSS(loss function) for all points that lie within the circle given by Œ≤1¬≤ + Œ≤2¬≤ ‚â§ s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, <b>for lasso, the equation becomes,|Œ≤1|+|Œ≤2|‚â§ s.</b> This implies that lasso coefficients have the smallest RSS(loss function) for all points that lie within the diamond given by |Œ≤1|+|Œ≤2|‚â§ s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image below describes these equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/reg_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The above image shows the constraint functions(green areas), for lasso(left) and ridge regression(right), along with contours for RSS(red ellipse).</b> Points on the ellipse share the value of RSS. For a very large value of s, the green regions will contain the center of the ellipse, making coefficient estimates of both regression techniques, equal to the least squares estimates. But, this is not the case in the above image. In this case, the lasso and ridge regression coefficient estimates are given by the Ô¨Årst point at which an ellipse contacts the constraint region. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coeÔ¨Écient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coeÔ¨Écients will equal zero.</b> In higher dimensions(where parameters are much more than 2), many of the coeÔ¨Écient estimates may equal zero simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark style='background-color:lightblue;'><b>This sheds light on the obvious disadvantage of ridge regression, which is model interpretability.</b> It will shrink the coefficients for least important predictors, very close to zero. But it will never make them exactly zero. In other words, the final model will include all predictors. However, in the case of the lasso, the L1 penalty has the eÔ¨Äect of forcing some of the coeÔ¨Écient estimates to be exactly equal to zero when the tuning parameter Œª is suÔ¨Éciently large. <b>Therefore, the lasso method also performs variable selection and is said to yield sparse models.</b></mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does Regularization achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard least squares model tends to have some variance in it, i.e. this model won‚Äôt generalize well for a data set different than its training data. <b>Regularization, significantly reduces the variance of the model, without substantial increase in its bias.</b> So the tuning parameter Œª, used in the regularization techniques described above, controls the impact on bias and variance. As the value of Œª rises, it reduces the value of coefficients and thus reducing the variance. <b>Till a point, this increase in Œª is beneficial as it is only reducing the variance(hence avoiding overfitting), without loosing any important properties in the data.</b> But after certain value, the model starts loosing important properties, giving rise to bias in the model and thus underfitting. Therefore, the value of Œª should be carefully selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the basic you will need, to get started with Regularization. It is a useful technique that can help in improving the accuracy of your regression models. A popular library for implementing these algorithms is <b><u>Scikit-Learn.</u> It has a wonderful api that can get your model up an running with just a few lines of code in python.</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
