{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Nearest Neighbour is a simplest algorithm that stores all the available cases and classifies the new data or cases based on a similarity measure. It is mostly used to classifies a data point based on how its neighbours are classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents\n",
    " #### 1. A simple example to understand the intution behind KNN\n",
    " #### 2. How does the KNN algorithm works?\n",
    " #### 3. Methods of calculating the distance between points\n",
    " #### 4. How to choose the k factor?\n",
    " #### 5. Pro and Cons of KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A sample example to understand the intution beind KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with a simple example. Consider the following table- it consist of the height, age and weight(target) value for 10 people. As you can see, the weight of value of ID11 is missing. We need to predict the weight of this person based on their height and age.\n",
    "Note:- This data in table is not real. it is just used for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/KNN_0.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more clear understanding of this, below is the plot of height versus age from the above table:\n",
    "<img src='img/Knn_1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, the y-axis represent the height of a person (in feet) and the x-axis represents the age (in years). The points are numbered according to the ID values. The yellow point (ID 11) is our test point.\n",
    "If i ask you to identify the weight if ID11 based on the plot, what would be your answer? You would likely say that since ID11 is <b>closer</b> to points 5 and 1, so it must have a weight similar to these IDs, probably between 72-77 kgs(weights of ID1 and ID5 from the table). That actually makes sense, but how do you think the algorithm predicts the values? we will find that out in this article "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. How does the KNN algorithm works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Calculate Similarity based on distance function\n",
    "\n",
    "There are many distance functions but Euclidean is the most commonly used measure. It is mainly used when data is continuous. Manhattan distance is also very common for continuous variables.\n",
    "\n",
    "<img src='img/Knn_2.png'>\n",
    "\n",
    "The idea to use distance measure is to find the distance (similarity) between new sample and training cases and then finds the k-closest peoples to new people in terms of Height and Age\n",
    "#### New guy has Height 5.5 and Age 38 \n",
    "Euclidean distance between first observation and new observation is as folows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " = SQRT((5-5.11)^2+(45-26)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will calculate distance of all the training cases with new case and calculates the rank in terms of distance. The smallest distance value will be ranked 1 and considered as nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Find K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Let k be 3.</b> Then the algorithm searches for the 3 people closest to new one,i.e most similar to new guy in terms of attributes, and see what categories those 3 guys were in. If 2 of them are of same color the new one belongs to that class. See the calculation shown in the snapshot below -\n",
    "<img src='img/Knn_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the new guy is from the group of blue colored points and it's weight would be the average of it's k nearest neighbores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Knn_4.png'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "= 77+72+60/3\n",
    "= 66.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of KNN\n",
    "\n",
    "#### 1. Standardization\n",
    "\n",
    "When independent variables in training data are measures in different units, it is important to standardize variables before calculating distance. For example, if one\n",
    "variable is based on Age in years and Height in feets , and the other is based on weight in kgs the height will infulence more on the distance calculation. In order to make them comparable we need to standardize them which can be done by any of the following methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Knn_5.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After standardization, 3rd closest value got changed as height was dominating earlier before standardization. Hence, it is important to standardize predictors before running K-nearest neighbor algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Knn_6.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low k-value is sensitive to outliers and a higher K-value is more resilient to outliers as it considers more voters to decide prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Methods of calculating the distance between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>first step</b> is to calculate the distance between the new point and each training point. There are various methods for calculating this distance, of which the most commonly known methods are – Euclidian, Manhattan (for continuous) and Hamming distance (for categorical).\n",
    "\n",
    " 1. <b>Euclidean Distance:</b> Euclidean distance is calculated as the square root of the sum of the squared differences between a new point (x) and an existing point (y).\n",
    " 2. <b>Manhattan Distance:</b> This is the distance between real vectors using the sum of their absolute difference.\n",
    "\n",
    "<img src='img/Knn_7.png'>\n",
    "\n",
    " 3. <b>Hamming Distance:</b> It is used for categorical variables. If the value (x) and the value (y) are the same, the distance D will be equal to 0 . Otherwise D=1.\n",
    " \n",
    "<img src='img/Knn_8.png'>\n",
    "\n",
    "Once the distance of a new observation from the points in our training set has been measured, the next step is to pick the closest points. The number of points to be considered is defined by the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to choose the k factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>second step</b> is to select the k value. This determines the number of neighbors we look at when we assign a value to any new observation.\n",
    "\n",
    "In our example, for a value k = 3, the closest points are ID1, ID5 and ID6.\n",
    "\n",
    "<img src='img/Knn_9.png'>\n",
    "<img src='img/Knn_10.png'>\n",
    "\n",
    "The prediction of weight for ID11 will be:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ID11 = (77+72+60)/3 \n",
    "\n",
    "ID11 = 69.66 kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the value of k=5, the closest point will be ID1, ID4, ID5, ID6, ID10.\n",
    "\n",
    "<img src='img/Knn_11.png'>\n",
    "<img src='img/Knn_12.png'>\n",
    "\n",
    "The prediction for ID11 will be :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ID 11 =  (77+59+72+60+58)/5 \n",
    "\n",
    "ID 11 = 65.2 kg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that based on the k value, the final result tends to change. Then how can we figure out the optimum value of k? Let us decide it based on the error calculation for our train and validation set (after all, minimizing the error is our final goal!).\n",
    "\n",
    "Have a look at the below graphs for training error and validation error for different values of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Knn_13.png'>\n",
    "<img src='img/Knn_14.png'>\n",
    "\n",
    "For a very low value of k (suppose k=1), the model overfits on the training data, which leads to a high error rate on the validation set. On the other hand, for a high value of k, the model performs poorly on both train and validation set. If you observe closely, the validation error curve reaches a minima at a value of k = 9. This value of k is the optimum value of the model (it will vary for different datasets). This curve is known as an <b>‘elbow curve‘</b> (because it has a shape like an elbow) and is usually used to determine the k value.\n",
    "\n",
    "You can also use the grid search technique to find the best k value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pros and Cons of KNN\n",
    "\n",
    "### Pros\n",
    "\n",
    "    1. Easy to understand\n",
    "    2. No assumptions about data\n",
    "    3. Can be applied to both classification and regression\n",
    "    4. Works easily on multi-class problems\n",
    "    \n",
    "### Cons\n",
    "    1. Memory Intensive / Computationally expensive\n",
    "    2. Sensitive to scale of data\n",
    "    3. Not work well on rare event (skewed) target variable\n",
    "    4. Struggle when high number of independent variables.\n",
    "    \n",
    "<i>For many given problem, a small value of k will be lead to a large variance in predictions. Alternatively, setting k to a large value may lead to a large model bias.</i>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
